{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**The Model**"
      ],
      "metadata": {
        "id": "bxpMHIKXIt8W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgeElJ9IIZwP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from nltk.corpus import wordnet\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Attention\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import nltk\n",
        "\n",
        "# Ensure nltk WordNet is downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_excel(\"Training_Datasets.xlsx\")\n",
        "test_data = pd.read_excel(\"Testing_Datasets.xlsx\")\n",
        "\n",
        "# Function for synonym replacement in the \"Notes\" column\n",
        "def augment_text_with_synonyms(text, num_replacements=2):\n",
        "    words = text.split()\n",
        "    for _ in range(num_replacements):\n",
        "        word_idx = random.randint(0, len(words) - 1)\n",
        "        synonyms = wordnet.synsets(words[word_idx])\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            if synonym != words[word_idx]:\n",
        "                words[word_idx] = synonym\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply text augmentation on \"Notes\" and \"Merchant Name\"\n",
        "train_data_augmented = train_data.copy()\n",
        "train_data_augmented['Notes'] = train_data_augmented['Notes'].apply(\n",
        "    lambda x: augment_text_with_synonyms(str(x), num_replacements=2)\n",
        ")\n",
        "train_data_augmented['Merchant Name'] = train_data_augmented['Merchant Name'].apply(\n",
        "    lambda x: augment_text_with_synonyms(str(x), num_replacements=1)\n",
        ")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_data_aligned(train_df, test_df):\n",
        "    # Processing datetime-related features\n",
        "    for df in [train_df, test_df]:\n",
        "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "        df['Year'] = df['Datetime'].dt.year\n",
        "        df['Month'] = df['Datetime'].dt.month\n",
        "        df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
        "        df['DayOfMonth'] = df['Datetime'].dt.day\n",
        "        df['WeekOfYear'] = df['Datetime'].dt.isocalendar().week\n",
        "        df['IsWeekend'] = df['Datetime'].dt.dayofweek.isin([5, 6]).astype(int)\n",
        "\n",
        "        # Convert 'Notes' and 'Merchant Name' to strings\n",
        "        df['Notes'] = df['Notes'].astype(str)\n",
        "        df['Merchant Name'] = df['Merchant Name'].astype(str)\n",
        "\n",
        "    # TF-IDF for 'Notes' column\n",
        "    tfidf_vectorizer_notes = TfidfVectorizer(max_features=150, ngram_range=(1, 2))\n",
        "    tfidf_train_notes = tfidf_vectorizer_notes.fit_transform(train_df['Notes']).toarray()\n",
        "    tfidf_test_notes = tfidf_vectorizer_notes.transform(test_df['Notes']).toarray()\n",
        "\n",
        "    # TF-IDF for 'Merchant Name' column\n",
        "    tfidf_vectorizer_merchant = TfidfVectorizer(max_features=150)\n",
        "    tfidf_train_merchant = tfidf_vectorizer_merchant.fit_transform(train_df['Merchant Name']).toarray()\n",
        "    tfidf_test_merchant = tfidf_vectorizer_merchant.transform(test_df['Merchant Name']).toarray()\n",
        "\n",
        "    # Creating DataFrames from the TF-IDF arrays\n",
        "    tfidf_train_df = pd.DataFrame(np.hstack((tfidf_train_notes, tfidf_train_merchant)),\n",
        "                                  columns=[f\"TFIDF_N_{i}\" for i in range(tfidf_train_notes.shape[1])] +\n",
        "                                          [f\"TFIDF_M_{i}\" for i in range(tfidf_train_merchant.shape[1])])\n",
        "    tfidf_test_df = pd.DataFrame(np.hstack((tfidf_test_notes, tfidf_test_merchant)),\n",
        "                                 columns=[f\"TFIDF_N_{i}\" for i in range(tfidf_test_notes.shape[1])] +\n",
        "                                         [f\"TFIDF_M_{i}\" for i in range(tfidf_test_merchant.shape[1])])\n",
        "\n",
        "    # One-hot encoding for categorical columns\n",
        "    categorical_train = pd.get_dummies(train_df[['Transaction Type', 'Payment Method']])\n",
        "    categorical_test = pd.get_dummies(test_df[['Transaction Type', 'Payment Method']])\n",
        "\n",
        "    categorical_train, categorical_test = categorical_train.align(categorical_test, join='outer', axis=1, fill_value=0)\n",
        "\n",
        "    # Extracting numerical features and log transformation\n",
        "    numerical_features = ['Amount', 'Year', 'Month', 'DayOfWeek', 'DayOfMonth', 'WeekOfYear', 'IsWeekend']\n",
        "    numerical_train = train_df[numerical_features].copy()\n",
        "    numerical_test = test_df[numerical_features].copy()\n",
        "\n",
        "    numerical_train['Amount_Log'] = np.log1p(numerical_train['Amount'])\n",
        "    numerical_test['Amount_Log'] = np.log1p(numerical_test['Amount'])\n",
        "\n",
        "    # Combining the features\n",
        "    train_combined = pd.concat([numerical_train, categorical_train, tfidf_train_df], axis=1)\n",
        "    test_combined = pd.concat([numerical_test, categorical_test, tfidf_test_df], axis=1)\n",
        "\n",
        "    return train_combined, test_combined\n",
        "\n",
        "X_train_full, X_test_full = preprocess_data_aligned(train_data, test_data)\n",
        "X_train_augmented, _ = preprocess_data_aligned(train_data_augmented, test_data)\n",
        "\n",
        "# Scaling the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_full)\n",
        "X_test_scaled = scaler.transform(X_test_full)\n",
        "X_train_scaled_augmented = scaler.transform(X_train_augmented)\n",
        "\n",
        "# Combining original and augmented data\n",
        "X_train_combined = np.vstack([X_train_scaled, X_train_scaled_augmented])\n",
        "\n",
        "# Encoding target labels\n",
        "le_category = LabelEncoder()\n",
        "y_train = le_category.fit_transform(train_data['Category'])\n",
        "y_test = le_category.transform(test_data['Category'])\n",
        "\n",
        "y_train_onehot = to_categorical(y_train)\n",
        "y_test_onehot = to_categorical(y_test)\n",
        "\n",
        "y_train_combined = np.vstack([y_train_onehot, y_train_onehot])\n",
        "\n",
        "# Adjusted model with increased complexity\n",
        "def create_complex_model(input_dim, output_dim):\n",
        "    model = Sequential([\n",
        "        Dense(1024, kernel_regularizer=l1_l2(l1=1e-4, l2=1e-3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(negative_slope=0.2),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(512, kernel_regularizer=l1_l2(l1=1e-4, l2=1e-3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(negative_slope=0.2),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(256, kernel_regularizer=l1_l2(l1=1e-4, l2=1e-3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(negative_slope=0.2),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(128, kernel_regularizer=l1_l2(l1=1e-4, l2=1e-3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(negative_slope=0.2),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Early stopping and learning rate scheduler\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=15,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Train the adjusted model\n",
        "complex_model = create_complex_model(X_train_combined.shape[1], y_train_combined.shape[1])\n",
        "complex_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_complex = complex_model.fit(\n",
        "    X_train_combined, y_train_combined,\n",
        "    epochs=300,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_loss_complex, test_accuracy_complex = complex_model.evaluate(X_test_scaled, y_test_onehot, verbose=0)\n",
        "print(f\"Test Accuracy (Complex Model): {test_accuracy_complex:.4f}\")\n",
        "print(f\"Test Loss (Complex Model): {test_loss_complex:.4f}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_complex.history['accuracy'], label='Training Accuracy (Complex)')\n",
        "plt.plot(history_complex.history['val_accuracy'], label='Validation Accuracy (Complex)')\n",
        "plt.title('Model Accuracy (Complex)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_complex.history['loss'], label='Training Loss (Complex)')\n",
        "plt.plot(history_complex.history['val_loss'], label='Validation Loss (Complex)')\n",
        "plt.title('Model Loss (Complex)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Save to .keras**"
      ],
      "metadata": {
        "id": "nsEXf3edIzqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "complex_model.save('categorization_model.keras')"
      ],
      "metadata": {
        "id": "qBxEAfBWJHNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model from the .keras file\n",
        "loaded_model = load_model('categorization_model.keras')\n",
        "\n",
        "# Check the model's structure\n",
        "loaded_model.summary()"
      ],
      "metadata": {
        "id": "fIOejULZJK43"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}